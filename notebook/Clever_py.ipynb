{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clever_py\r\n",
        "\r\n",
        "__NOTE:__ This notebook currently only processes the Student Clever data (rather than students, teachers, and staff data).\r\n",
        "\r\n",
        "This notebook can be modified as needed to account for processing more Clever user data (being teachers and/or staff)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\r\n",
        "\r\n",
        "class Clever(BaseOEAModule):\r\n",
        "    def __init__(self, source_folder='clever'):\r\n",
        "        BaseOEAModule.__init__(self, source_folder)\r\n",
        "\r\n",
        "        self.schemas['daily_participation'] = [['date', 'date', 'no-op'],\r\n",
        "                                            ['sis_id', 'string', 'hash'],\r\n",
        "                                            ['clever_user_id', 'string', 'hash'],\r\n",
        "                                            ['clever_school_id', 'string', 'no-op'],\r\n",
        "                                            ['school_name', 'string', 'no-op'],\r\n",
        "                                            ['active', 'boolean', 'no-op'],\r\n",
        "                                            ['num_logins', 'integer', 'no-op'],\r\n",
        "                                            ['num_resources_accessed', 'integer', 'no-op']]\r\n",
        "        self.schemas['resource_usage'] = [['date', 'date', 'no-op'], \r\n",
        "                                            ['sis_id', 'string', 'hash'],\r\n",
        "                                            ['clever_user_id', 'string', 'hash'],\r\n",
        "                                            ['clever_school_id', 'string', 'no-op'],\r\n",
        "                                            ['school_name', 'string', 'no-op'],\r\n",
        "                                            ['resource_type', 'string', 'no-op'],\r\n",
        "                                            ['resource_name', 'string', 'no-op'],\r\n",
        "                                            ['resource_id', 'string', 'no-op'],\r\n",
        "                                            ['num_access', 'integer', 'no-op']]\r\n",
        "\r\n",
        "    def ingest(self):\r\n",
        "        \"\"\"Processes clever data from stage1 into stage2 using structured streaming within the defined functions below.\"\"\"\r\n",
        "        logger.info(\"Processing clever data from: \" + self.stage1np)\r\n",
        "\r\n",
        "        items = mssparkutils.fs.ls(self.stage1np)\r\n",
        "        for item in items:\r\n",
        "            if item.name == \"daily-participation\":\r\n",
        "                self._process_clever_stage1_data(table_name='daily-participation', folder='/*', partition_column='date')\r\n",
        "            elif item.name == \"resource-usage\":\r\n",
        "                self._process_clever_stage1_data(table_name='resource-usage', folder='/*', partition_column='date')\r\n",
        "            else:\r\n",
        "                logger.info(\"No defined function for processing this clever data\")\r\n",
        "        \r\n",
        "        logger.info(\"Finished processing clever data from stage 1 to stage 2\")\r\n",
        "\r\n",
        "    def _process_clever_stage1_data(self, table_name=None, folder=None, partition_column=None):\r\n",
        "        \"\"\" Processes any clever data table from stage1 to stage2 using structured streaming. \"\"\"\r\n",
        "        \r\n",
        "        # change new table name to match OEA conventions\r\n",
        "        new_table_name = table_name.replace('-', '_')\r\n",
        "\r\n",
        "        source_path = f'{self.stage1np}/{table_name}'\r\n",
        "        p_destination_path = f'{self.stage2p}/{new_table_name}_pseudo'\r\n",
        "        np_destination_path = f'{self.stage2np}/{new_table_name}_lookup'\r\n",
        "\r\n",
        "        logger.info(f'Processing incremental data from: {source_path} and writing out to: {p_destination_path}')\r\n",
        "        \r\n",
        "        clever_spark_schema = oea.to_spark_schema(self.schemas[new_table_name])\r\n",
        "        # read in the raw data using structured streaming\r\n",
        "        df = spark.readStream.csv(source_path + folder + '/*.csv', header='true', schema=clever_spark_schema)\r\n",
        "\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas[new_table_name])\r\n",
        "\r\n",
        "        # write out the resulting, final tables \r\n",
        "        if len(df_pseudo.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2p')\r\n",
        "        else:\r\n",
        "            df_pseudo = df_pseudo.withColumn('year', F.year(F.col(partition_column))).withColumn('month', F.month(F.col(partition_column)))\r\n",
        "            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path +  '/_checkpoints_p').partitionBy('year', 'month')\r\n",
        "            query = query.start(p_destination_path)\r\n",
        "            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "        \r\n",
        "        if len(df_lookup.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2np')\r\n",
        "        else:\r\n",
        "            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints_np')\r\n",
        "            query2 = query2.start(np_destination_path)\r\n",
        "            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}